{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e8a18cc",
   "metadata": {},
   "source": [
    "# Prétraitement pour DEVA\n",
    "Ce notebook prépare les données pour l’architecture DEVA : extraction des features audio/visuelles, génération des descriptions émotionnelles, et encodage textuel avancé."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c4c9e050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "MOSI path: ./MOSI\n",
      "Output path: ./preprocessed_data\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import cv2\n",
    "import os\n",
    "from glob import glob\n",
    "import re\n",
    "from mmsdk import mmdatasdk\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Paths\n",
    "mosi_path = \"./MOSI\"\n",
    "output_path = \"./preprocessed_data\"\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "print(f\"MOSI path: {mosi_path}\")\n",
    "print(f\"Output path: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afafa82b",
   "metadata": {},
   "source": [
    "## 1. Load MOSI Labels from SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4528b49a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading MOSI labels from SDK...\n",
      "\u001b[94m\u001b[1m[2025-12-02 15:26:40.509] | Status  | \u001b[0mDownloading from http://immortal.multicomp.cs.cmu.edu/CMU-MOSI/labels/CMU_MOSI_Opinion_Labels.csd to ./MOSI/SDK_data/CMU_MOSI_Opinion_Labels.csd...\n",
      "\u001b[94m\u001b[1m[2025-12-02 15:26:40.509] | Status  | \u001b[0mDownloading from http://immortal.multicomp.cs.cmu.edu/CMU-MOSI/labels/CMU_MOSI_Opinion_Labels.csd to ./MOSI/SDK_data/CMU_MOSI_Opinion_Labels.csd...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m\u001b[1m[2025-12-02 15:26:41.313] | Success | \u001b[0mDownload complete!\n",
      "\u001b[92m\u001b[1m[2025-12-02 15:26:41.332] | Success | \u001b[0mComputational sequence read from file ./MOSI/SDK_data/CMU_MOSI_Opinion_Labels.csd ...\n",
      "\u001b[94m\u001b[1m[2025-12-02 15:26:41.356] | Status  | \u001b[0mChecking the integrity of the <Opinion Segment Labels> computational sequence ...\n",
      "\u001b[94m\u001b[1m[2025-12-02 15:26:41.356] | Status  | \u001b[0mChecking the format of the data in <Opinion Segment Labels> computational sequence ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m\u001b[1m[2025-12-02 15:26:41.374] | Success | \u001b[0m<Opinion Segment Labels> computational sequence data in correct format.\n",
      "\u001b[94m\u001b[1m[2025-12-02 15:26:41.374] | Status  | \u001b[0mChecking the format of the metadata in <Opinion Segment Labels> computational sequence ...\n",
      "\u001b[93m\u001b[1m[2025-12-02 15:26:41.374] | Warning | \u001b[0m<Opinion Segment Labels> computational sequence does not have all the required metadata ... continuing \n",
      "\u001b[92m\u001b[1m[2025-12-02 15:26:41.374] | Success | \u001b[0mDataset initialized successfully ... \n",
      "✓ Loaded labels for 93 videos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# Load MOSI labels using SDK\n",
    "print(\"Loading MOSI labels from SDK...\")\n",
    "dataset = mmdatasdk.mmdataset(mmdatasdk.cmu_mosi.labels, os.path.join(mosi_path, 'SDK_data'))\n",
    "opinion_labels_sdk = dataset.computational_sequences['Opinion Segment Labels']\n",
    "\n",
    "print(f\"✓ Loaded labels for {len(opinion_labels_sdk.data)} videos\")\n",
    "\n",
    "def get_sentiment_label(video_id, segment_id):\n",
    "    \"\"\"Retrieve sentiment label from SDK.\"\"\"\n",
    "    try:\n",
    "        key = video_id\n",
    "        if key in opinion_labels_sdk.data:\n",
    "            intervals = opinion_labels_sdk.data[key]['intervals']\n",
    "            features = opinion_labels_sdk.data[key]['features']\n",
    "            segment_idx = int(segment_id) - 1\n",
    "            if 0 <= segment_idx < len(features):\n",
    "                sentiment_score = features[segment_idx][0]\n",
    "                return float(sentiment_score)\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb24ac6c",
   "metadata": {},
   "source": [
    "## 2. Initialize Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "27cec7cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT model...\n",
      "✓ BERT loaded\n",
      "✓ TextEncoder initialized (matches training architecture)\n",
      "✓ Image encoder initialized\n",
      "✓ BERT loaded\n",
      "✓ TextEncoder initialized (matches training architecture)\n",
      "✓ Image encoder initialized\n"
     ]
    }
   ],
   "source": [
    "# Initialize BERT\n",
    "print(\"Loading BERT model...\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "bert = BertModel.from_pretrained(\"bert-base-uncased\").to(device)\n",
    "bert.eval()\n",
    "print(\"✓ BERT loaded\")\n",
    "\n",
    "# TextEncoder class (matches training notebook architecture)\n",
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, bert_dim=768, num_output_tokens=8, nhead=8):\n",
    "        super().__init__()\n",
    "        self.bert_dim = bert_dim\n",
    "        self.T = num_output_tokens\n",
    "        \n",
    "        # Token spécial E_m (learnable)\n",
    "        self.special_token = nn.Parameter(torch.randn(1, 1, bert_dim))\n",
    "        \n",
    "        # TransformerEncoderLayer\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=bert_dim,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=2048,\n",
    "            dropout=0.1,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=1)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Obtenir la séquence brute de BERT\n",
    "        with torch.no_grad():\n",
    "            bert_output = bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            bert_sequence = bert_output.last_hidden_state  # [batch_size, seq_len, 768]\n",
    "        \n",
    "        batch_size = bert_sequence.size(0)\n",
    "        seq_len = bert_sequence.size(1)\n",
    "        \n",
    "        # Ajouter le token spécial E_m en tête\n",
    "        special_token_expanded = self.special_token.expand(batch_size, -1, -1)\n",
    "        sequence_with_token = torch.cat([special_token_expanded, bert_sequence], dim=1)\n",
    "        \n",
    "        # Appliquer le TransformerEncoderLayer\n",
    "        enhanced_sequence = self.transformer_encoder(sequence_with_token)\n",
    "        \n",
    "        # Ne conserver que les T=8 premiers tokens (ou padding si moins)\n",
    "        if enhanced_sequence.size(1) >= self.T:\n",
    "            output_sequence = enhanced_sequence[:, :self.T, :]  # [batch_size, 8, 768]\n",
    "        else:\n",
    "            # Si moins de T tokens, faire du padding\n",
    "            padding_size = self.T - enhanced_sequence.size(1)\n",
    "            padding = torch.zeros(batch_size, padding_size, self.bert_dim, device=enhanced_sequence.device)\n",
    "            output_sequence = torch.cat([enhanced_sequence, padding], dim=1)\n",
    "        \n",
    "        return output_sequence\n",
    "\n",
    "# Initialize TextEncoder\n",
    "text_encoder = TextEncoder().to(device)\n",
    "text_encoder.eval()\n",
    "print(\"✓ TextEncoder initialized (matches training architecture)\")\n",
    "\n",
    "# Simple Image Encoder\n",
    "class ImgEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(3, 8, 3, 1, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d(1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x / 255.\n",
    "        x = torch.tensor(x).permute(2, 0, 1).unsqueeze(0).float().to(device)\n",
    "        return self.model(x).flatten()\n",
    "\n",
    "img_encoder = ImgEncoder().to(device)\n",
    "img_encoder.eval()\n",
    "print(\"✓ Image encoder initialized\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a73d426",
   "metadata": {},
   "source": [
    "## 3. Feature Extraction Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d84d2782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Feature extraction functions updated (OpenFace-only, no fallbacks)\n"
     ]
    }
   ],
   "source": [
    "sr = 16000\n",
    "\n",
    "import subprocess\n",
    "\n",
    "# ===================== CONFIG FOR OPENFACE =====================\n",
    "OPENFACE_OUTPUT_DIR = \"./openface_output\"\n",
    "OPENFACE_BIN = \"./OpenFace/build/bin/FeatureExtraction\"  # Adjust if different\n",
    "OPENFACE_AUTO = True  # Attempt to run OpenFace when AU CSV missing\n",
    "\n",
    "# ===================== AUDIO FEATURE EXTRACTION =====================\n",
    "def extract_audio_features(audio, sr=16000):\n",
    "    \"\"\"Extract emotional audio features.\"\"\"\n",
    "    pitches, magnitudes = librosa.piptrack(y=audio, sr=sr)\n",
    "    pitch = np.mean([pitches[magnitudes[:, t].argmax(), t]\n",
    "                     for t in range(pitches.shape[1])\n",
    "                     if magnitudes[:, t].max() > 0] or [0])\n",
    "\n",
    "    rms = librosa.feature.rms(y=audio)[0]\n",
    "    loudness = np.mean(rms)\n",
    "\n",
    "    pitch_values = [pitches[magnitudes[:, t].argmax(), t]\n",
    "                    for t in range(pitches.shape[1])\n",
    "                    if magnitudes[:, t].max() > 0]\n",
    "    jitter = np.std(pitch_values) if len(pitch_values) > 1 else 0\n",
    "    shimmer = np.std(rms)\n",
    "\n",
    "    return {\n",
    "        'pitch': float(pitch),\n",
    "        'loudness': float(loudness),\n",
    "        'jitter': float(jitter),\n",
    "        'shimmer': float(shimmer)\n",
    "    }\n",
    "\n",
    "# ===================== AUDIO DESCRIPTION =====================\n",
    "def audio_description(pitch, loudness, jitter, shimmer):\n",
    "    \"\"\"Convert audio features to emotional description (coarse).\"\"\"\n",
    "    pitch_level = \"high\" if pitch > 200 else \"low\" if pitch > 100 else \"very low\"\n",
    "    loudness_level = \"loud\" if loudness > 0.1 else \"moderate\" if loudness > 0.05 else \"quiet\"\n",
    "    jitter_level = \"high variation\" if jitter > 20 else \"stable\"\n",
    "    shimmer_level = \"variable amplitude\" if shimmer > 0.02 else \"steady amplitude\"\n",
    "\n",
    "    return (\n",
    "        f\"The speaker used {pitch_level} pitch with {loudness_level} volume. \"\n",
    "        f\"The voice shows {jitter_level} in pitch and {shimmer_level}.\"\n",
    "    )\n",
    "\n",
    "# ===================== VISUAL AU DESCRIPTION SUPPORT =====================\n",
    "AU_DESCRIPTIONS = {\n",
    "    'AU1': 'raised inner brow', 'AU2': 'raised outer brow', 'AU4': 'lowered brow',\n",
    "    'AU5': 'upper lid raise', 'AU6': 'cheek raise', 'AU7': 'lid tightener',\n",
    "    'AU9': 'nose wrinkle', 'AU10': 'upper lip raise', 'AU12': 'lip corner pull',\n",
    "    'AU15': 'lip corner depress', 'AU17': 'chin raise', 'AU20': 'lip stretch',\n",
    "    'AU23': 'lip tightener', 'AU25': 'lips part', 'AU26': 'jaw drop', 'AU45': 'blink'\n",
    "}\n",
    "\n",
    "# --------------------- OpenFace CSV Parsing ---------------------\n",
    "\n",
    "def extract_aus_openface(video_id, seg_id, openface_dir=OPENFACE_OUTPUT_DIR):\n",
    "    \"\"\"Parse OpenFace CSV for a given video segment and return averaged AU intensities.\n",
    "    Looks for `<video_id>_<seg_id>.csv` or nested path.\n",
    "    Returns dict {AUxx: intensity} with keys sans `_r` suffix.\n",
    "    \"\"\"\n",
    "    base = f\"{video_id}_{seg_id}\"\n",
    "    candidates = [\n",
    "        os.path.join(openface_dir, f\"{base}.csv\"),\n",
    "        os.path.join(openface_dir, base, f\"{base}.csv\")\n",
    "    ]\n",
    "    for path in candidates:\n",
    "        if os.path.exists(path):\n",
    "            try:\n",
    "                df_au = pd.read_csv(path)\n",
    "                if 'success' in df_au.columns:\n",
    "                    df_au = df_au[df_au['success'] == 1]\n",
    "                au_cols = [c for c in df_au.columns if c.startswith('AU') and c.endswith('_r')]\n",
    "                if not au_cols:\n",
    "                    return {}\n",
    "                mean_intensities = df_au[au_cols].mean().to_dict()\n",
    "                return {k.replace('_r',''): float(v) for k, v in mean_intensities.items()}\n",
    "            except Exception:\n",
    "                return {}\n",
    "    return {}\n",
    "\n",
    "# --------------------- OpenFace Invocation ---------------------\n",
    "\n",
    "def run_openface(video_path, output_dir=OPENFACE_OUTPUT_DIR, binary_path=OPENFACE_BIN):\n",
    "    \"\"\"Run OpenFace FeatureExtraction on a single video if binary exists.\n",
    "    Returns True if run attempted (success or failure), False if skipped.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(binary_path):\n",
    "        return False\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    cmd = [binary_path, '-f', video_path, '-out_dir', output_dir, '-aus']\n",
    "    try:\n",
    "        res = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, timeout=120)\n",
    "        if res.returncode != 0:\n",
    "            print(f\"[OpenFace] Error ({res.returncode}) on {os.path.basename(video_path)}: {res.stderr.decode(errors='ignore')[:200]}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"[OpenFace] Exception running on {video_path}: {e}\")\n",
    "        return True\n",
    "\n",
    "# --------------------- Visual Description ---------------------\n",
    "\n",
    "def visual_description(aus_data, activation_threshold=0.5):\n",
    "    if isinstance(aus_data, dict):\n",
    "        active_aus = [au for au, val in aus_data.items() if val >= activation_threshold]\n",
    "    elif isinstance(aus_data, (list, tuple)):\n",
    "        active_aus = list(aus_data)\n",
    "    else:\n",
    "        active_aus = []\n",
    "\n",
    "    if not active_aus:\n",
    "        return \"The person shows a neutral expression with no significant facial movement.\"\n",
    "\n",
    "    descriptions = [AU_DESCRIPTIONS.get(au, au) for au in active_aus]\n",
    "\n",
    "    if len(descriptions) == 1:\n",
    "        return f\"The person shows signs of: {descriptions[0]}.\"\n",
    "    elif len(descriptions) == 2:\n",
    "        return f\"The person shows signs of: {descriptions[0]} and {descriptions[1]}.\"\n",
    "    else:\n",
    "        desc_list = \", \".join(descriptions[:-1]) + f\", and {descriptions[-1]}\"\n",
    "        return f\"The person shows signs of: {desc_list}.\"\n",
    "\n",
    "# ===================== TEXT ENCODING =====================\n",
    "\n",
    "def encode_text_with_text_encoder(text):\n",
    "    tokens = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=128).to(device)\n",
    "    with torch.no_grad():\n",
    "        sequence = text_encoder(tokens['input_ids'], tokens['attention_mask'])\n",
    "        emb = sequence.mean(dim=1)\n",
    "    return emb.cpu().numpy().flatten()\n",
    "\n",
    "def encode_audio_mfcc(audio):\n",
    "    mfcc = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=20)\n",
    "    return mfcc.mean(axis=1)\n",
    "\n",
    "print(\"✓ Feature extraction functions updated (OpenFace-only, no fallbacks)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c56b2b",
   "metadata": {},
   "source": [
    "## 4. Data Loading & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "65baf833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Data preprocessing function updated (OpenFace-only; no fallbacks)\n"
     ]
    }
   ],
   "source": [
    "def parse_transcript_file(filepath):\n",
    "    \"\"\"Parse transcript file.\"\"\"\n",
    "    segments = []\n",
    "    with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if '_DELIM_' in line:\n",
    "                parts = line.split('_DELIM_', 1)\n",
    "                if len(parts) == 2:\n",
    "                    seg_id = parts[0].strip()\n",
    "                    text = parts[1].strip()\n",
    "                    segments.append({'id': seg_id, 'text': text})\n",
    "    return segments\n",
    "\n",
    "def extract_video_id(filename):\n",
    "    \"\"\"Extract video ID from filename.\"\"\"\n",
    "    base = os.path.splitext(os.path.basename(filename))[0]\n",
    "    parts = base.rsplit('_', 1)\n",
    "    return parts[0] if len(parts) > 1 else base\n",
    "\n",
    "def preprocess_mosi_dataset(max_samples=500, num_video_frames=8, au_threshold=0.5):\n",
    "    \"\"\"Preprocess MOSI dataset: audio/text features + OpenFace AU intensities (invoked automatically).\"\"\"\n",
    "    audio_dir = os.path.join(mosi_path, \"Audio\", \"WAV_16000\", \"Segmented\")\n",
    "    video_dir = os.path.join(mosi_path, \"Video\", \"Segmented\")\n",
    "    transcript_dir = os.path.join(mosi_path, \"Transcript\", \"Segmented\")\n",
    "\n",
    "    audio_files = sorted(glob(os.path.join(audio_dir, \"*.wav\")))\n",
    "    data_records = []\n",
    "\n",
    "    print(f\"\\nProcessing up to {max_samples} samples...\")\n",
    "    if OPENFACE_AUTO and not os.path.exists(OPENFACE_BIN):\n",
    "        print(f\"[Warning] OpenFace binary '{OPENFACE_BIN}' not found. Set OPENFACE_AUTO=False or adjust OPENFACE_BIN path.\")\n",
    "\n",
    "    for audio_file in tqdm(audio_files[:max_samples], desc=\"Processing samples\"):\n",
    "        try:\n",
    "            basename = os.path.basename(audio_file)\n",
    "            base_no_ext = os.path.splitext(basename)[0]\n",
    "            video_id = extract_video_id(audio_file)\n",
    "\n",
    "            seg_match = re.search(r'_(\\d+)$', base_no_ext)\n",
    "            if not seg_match:\n",
    "                continue\n",
    "            seg_id = seg_match.group(1)\n",
    "\n",
    "            sentiment_label = get_sentiment_label(video_id, seg_id)\n",
    "            if sentiment_label is None:\n",
    "                continue\n",
    "\n",
    "            audio, _ = librosa.load(audio_file, sr=sr)\n",
    "            audio_duration = len(audio) / sr\n",
    "            audio_mfcc = encode_audio_mfcc(audio)\n",
    "            audio_emotional_features = extract_audio_features(audio, sr)\n",
    "            audio_desc = audio_description(\n",
    "                audio_emotional_features['pitch'],\n",
    "                audio_emotional_features['loudness'],\n",
    "                audio_emotional_features['jitter'],\n",
    "                audio_emotional_features['shimmer']\n",
    "            )\n",
    "            audio_desc_embedding = encode_text_with_text_encoder(audio_desc)\n",
    "\n",
    "            video_file = os.path.join(video_dir, f\"{video_id}_{seg_id}.mp4\")\n",
    "            aus_intensities = extract_aus_openface(video_id, seg_id)\n",
    "            if not aus_intensities and OPENFACE_AUTO and os.path.exists(video_file):\n",
    "                run_attempted = run_openface(video_file)\n",
    "                if run_attempted:\n",
    "                    aus_intensities = extract_aus_openface(video_id, seg_id)\n",
    "\n",
    "            # mid-frame for simple CNN embedding\n",
    "            image = None\n",
    "            if os.path.exists(video_file):\n",
    "                cap = cv2.VideoCapture(video_file)\n",
    "                frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "                if frame_count > 0:\n",
    "                    cap.set(cv2.CAP_PROP_POS_FRAMES, frame_count // 2)\n",
    "                    ret, frame = cap.read()\n",
    "                    if ret:\n",
    "                        image = cv2.resize(frame, (64, 64))\n",
    "                        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "                cap.release()\n",
    "            if image is None:\n",
    "                image = np.zeros((64, 64, 3), dtype=np.uint8)\n",
    "            visual_cnn = img_encoder(image).detach().cpu().numpy()\n",
    "\n",
    "            visual_desc = visual_description(aus_intensities, activation_threshold=au_threshold)\n",
    "            visual_desc_embedding = encode_text_with_text_encoder(visual_desc)\n",
    "\n",
    "            transcript_file = os.path.join(transcript_dir, f\"{video_id}.annotprocessed\")\n",
    "            text = \"\"\n",
    "            if os.path.exists(transcript_file):\n",
    "                segments = parse_transcript_file(transcript_file)\n",
    "                for seg in segments:\n",
    "                    if seg['id'] == seg_id:\n",
    "                        text = seg['text']\n",
    "                        break\n",
    "            if not text:\n",
    "                text = f\"Segment {seg_id}\"\n",
    "            text_embedding = encode_text_with_text_encoder(text)\n",
    "\n",
    "            record = {\n",
    "                'video_id': video_id,\n",
    "                'segment_id': seg_id,\n",
    "                'text': text,\n",
    "                'text_embedding': text_embedding,\n",
    "                'audio_mfcc': audio_mfcc,\n",
    "                'audio_pitch': audio_emotional_features['pitch'],\n",
    "                'audio_loudness': audio_emotional_features['loudness'],\n",
    "                'audio_jitter': audio_emotional_features['jitter'],\n",
    "                'audio_shimmer': audio_emotional_features['shimmer'],\n",
    "                'audio_description': audio_desc,\n",
    "                'audio_desc_embedding': audio_desc_embedding,\n",
    "                'visual_cnn': visual_cnn,\n",
    "                'aus_intensities': aus_intensities,\n",
    "                'visual_description': visual_desc,\n",
    "                'visual_desc_embedding': visual_desc_embedding,\n",
    "                'audio_duration': audio_duration,\n",
    "                'sentiment_label': sentiment_label\n",
    "            }\n",
    "\n",
    "            data_records.append(record)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError processing {basename}: {e}\")\n",
    "            continue\n",
    "\n",
    "    return data_records\n",
    "\n",
    "print(\"✓ Data preprocessing function updated (OpenFace-only; no fallbacks)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065d40c7",
   "metadata": {},
   "source": [
    "## 5. Process Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3ce626a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "STARTING DATA PREPROCESSING\n",
      "======================================================================\n",
      "\n",
      "Processing up to 500 samples...\n",
      "[Warning] OpenFace binary './OpenFace/build/bin/FeatureExtraction' not found. Set OPENFACE_AUTO=False or adjust OPENFACE_BIN path.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing samples: 100%|██████████| 500/500 [01:09<00:00,  7.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Processed 500 samples successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Process the dataset (adjust max_samples as needed)\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STARTING DATA PREPROCESSING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "data_records = preprocess_mosi_dataset(max_samples=500)\n",
    "\n",
    "print(f\"\\n✓ Processed {len(data_records)} samples successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6fd3a4",
   "metadata": {},
   "source": [
    "## 6. Create DataFrame & Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0042e8d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "DATASET STATISTICS\n",
      "======================================================================\n",
      "Total samples: 500\n",
      "\n",
      "Columns: ['video_id', 'segment_id', 'text', 'text_embedding', 'audio_mfcc', 'audio_pitch', 'audio_loudness', 'audio_jitter', 'audio_shimmer', 'audio_description', 'audio_desc_embedding', 'visual_cnn', 'aus_intensities', 'visual_description', 'visual_desc_embedding', 'audio_duration', 'sentiment_label']\n",
      "\n",
      "Sentiment label statistics:\n",
      "  Min: -2.800\n",
      "  Max: 3.000\n",
      "  Mean: 0.417\n",
      "  Std: 1.451\n",
      "\n",
      "Label distribution:\n",
      "  Positive (>0): 280\n",
      "  Negative (<0): 197\n",
      "  Neutral (=0): 23\n",
      "\n",
      "======================================================================\n",
      "SAMPLE DATA (first 3 rows)\n",
      "======================================================================\n",
      "\n",
      "Sample 1:\n",
      "  Video ID: 03bSnISJMiM\n",
      "  Segment ID: 1\n",
      "  Text: ANYHOW IT WAS REALLY GOOD...\n",
      "  Audio Description: The speaker used high pitch with quiet volume. The voice shows high variation in pitch and steady amplitude.\n",
      "  Visual Description: The person shows a neutral expression with no significant facial movement.\n",
      "  Sentiment Label: +2.400\n",
      "  Text embedding shape: (768,)\n",
      "  Audio MFCC shape: (20,)\n",
      "  Visual CNN shape: (8,)\n",
      "\n",
      "Sample 2:\n",
      "  Video ID: 03bSnISJMiM\n",
      "  Segment ID: 10\n",
      "  Text: THERE IS SAD PART...\n",
      "  Audio Description: The speaker used high pitch with quiet volume. The voice shows high variation in pitch and steady amplitude.\n",
      "  Visual Description: The person shows a neutral expression with no significant facial movement.\n",
      "  Sentiment Label: -1.200\n",
      "  Text embedding shape: (768,)\n",
      "  Audio MFCC shape: (20,)\n",
      "  Visual CNN shape: (8,)\n",
      "\n",
      "Sample 3:\n",
      "  Video ID: 03bSnISJMiM\n",
      "  Segment ID: 11\n",
      "  Text: A LOT OF SAD PARTS...\n",
      "  Audio Description: The speaker used high pitch with quiet volume. The voice shows high variation in pitch and steady amplitude.\n",
      "  Visual Description: The person shows a neutral expression with no significant facial movement.\n",
      "  Sentiment Label: -0.500\n",
      "  Text embedding shape: (768,)\n",
      "  Audio MFCC shape: (20,)\n",
      "  Visual CNN shape: (8,)\n"
     ]
    }
   ],
   "source": [
    "# Create DataFrame\n",
    "df = pd.DataFrame(data_records)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DATASET STATISTICS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Total samples: {len(df)}\")\n",
    "print(f\"\\nColumns: {list(df.columns)}\")\n",
    "print(f\"\\nSentiment label statistics:\")\n",
    "print(f\"  Min: {df['sentiment_label'].min():.3f}\")\n",
    "print(f\"  Max: {df['sentiment_label'].max():.3f}\")\n",
    "print(f\"  Mean: {df['sentiment_label'].mean():.3f}\")\n",
    "print(f\"  Std: {df['sentiment_label'].std():.3f}\")\n",
    "print(f\"\\nLabel distribution:\")\n",
    "print(f\"  Positive (>0): {(df['sentiment_label'] > 0).sum()}\")\n",
    "print(f\"  Negative (<0): {(df['sentiment_label'] < 0).sum()}\")\n",
    "print(f\"  Neutral (=0): {(df['sentiment_label'] == 0).sum()}\")\n",
    "\n",
    "# Display sample\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SAMPLE DATA (first 3 rows)\")\n",
    "print(\"=\"*70)\n",
    "for idx in range(min(3, len(df))):\n",
    "    print(f\"\\nSample {idx+1}:\")\n",
    "    print(f\"  Video ID: {df.iloc[idx]['video_id']}\")\n",
    "    print(f\"  Segment ID: {df.iloc[idx]['segment_id']}\")\n",
    "    print(f\"  Text: {df.iloc[idx]['text'][:80]}...\")\n",
    "    print(f\"  Audio Description: {df.iloc[idx]['audio_description']}\")\n",
    "    print(f\"  Visual Description: {df.iloc[idx]['visual_description']}\")\n",
    "    print(f\"  Sentiment Label: {df.iloc[idx]['sentiment_label']:+.3f}\")\n",
    "    print(f\"  Text embedding shape: {df.iloc[idx]['text_embedding'].shape}\")\n",
    "    print(f\"  Audio MFCC shape: {df.iloc[idx]['audio_mfcc'].shape}\")\n",
    "    print(f\"  Visual CNN shape: {df.iloc[idx]['visual_cnn'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "34da80d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "SAVING PREPROCESSED DATA\n",
      "======================================================================\n",
      "✓ Saved pickle file: ./preprocessed_data/mosi_preprocessed.pkl\n",
      "✓ Saved metadata CSV: ./preprocessed_data/mosi_metadata.csv\n",
      "✓ Saved dataset info: ./preprocessed_data/dataset_info.pkl\n",
      "\n",
      "======================================================================\n",
      "PREPROCESSING COMPLETE!\n",
      "======================================================================\n",
      "\n",
      "Files saved in: ./preprocessed_data\n",
      "  - mosi_preprocessed.pkl (full dataset with embeddings)\n",
      "  - mosi_metadata.csv (metadata without embeddings)\n",
      "  - dataset_info.pkl (feature dimensions and info)\n"
     ]
    }
   ],
   "source": [
    "# Save DataFrame\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SAVING PREPROCESSED DATA\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Save as pickle (preserves numpy arrays)\n",
    "pickle_path = os.path.join(output_path, 'mosi_preprocessed.pkl')\n",
    "with open(pickle_path, 'wb') as f:\n",
    "    pickle.dump(df, f)\n",
    "print(f\"✓ Saved pickle file: {pickle_path}\")\n",
    "\n",
    "# Save metadata as CSV (without embeddings)\n",
    "metadata_df = df[['video_id', 'segment_id', 'text', 'audio_description', 'visual_description', \n",
    "                  'audio_pitch', 'audio_loudness', 'audio_jitter', 'audio_shimmer',\n",
    "                  'audio_duration', 'sentiment_label']].copy()\n",
    "csv_path = os.path.join(output_path, 'mosi_metadata.csv')\n",
    "metadata_df.to_csv(csv_path, index=False)\n",
    "print(f\"✓ Saved metadata CSV: {csv_path}\")\n",
    "\n",
    "# Save feature dimensions info\n",
    "info = {\n",
    "    'num_samples': len(df),\n",
    "    'text_embedding_dim': df.iloc[0]['text_embedding'].shape[0],\n",
    "    'audio_mfcc_dim': df.iloc[0]['audio_mfcc'].shape[0],\n",
    "    'visual_cnn_dim': df.iloc[0]['visual_cnn'].shape[0],\n",
    "    'audio_desc_embedding_dim': df.iloc[0]['audio_desc_embedding'].shape[0],\n",
    "    'visual_desc_embedding_dim': df.iloc[0]['visual_desc_embedding'].shape[0],\n",
    "    'columns': list(df.columns)\n",
    "}\n",
    "\n",
    "info_path = os.path.join(output_path, 'dataset_info.pkl')\n",
    "with open(info_path, 'wb') as f:\n",
    "    pickle.dump(info, f)\n",
    "print(f\"✓ Saved dataset info: {info_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PREPROCESSING COMPLETE!\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nFiles saved in: {output_path}\")\n",
    "print(f\"  - mosi_preprocessed.pkl (full dataset with embeddings)\")\n",
    "print(f\"  - mosi_metadata.csv (metadata without embeddings)\")\n",
    "print(f\"  - dataset_info.pkl (feature dimensions and info)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projet-multi-20251201",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
