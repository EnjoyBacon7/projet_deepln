{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e8a18cc",
   "metadata": {},
   "source": [
    "# Prétraitement pour DEVA\n",
    "Ce notebook prépare les données pour l’architecture DEVA : extraction des features audio/visuelles, génération des descriptions émotionnelles, et encodage textuel avancé."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c4c9e050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "MOSI path: /Users/camillebizeul/Downloads/Projet_Multi-20251201/MOSI\n",
      "Output path: /Users/camillebizeul/Downloads/Projet_Multi-20251201/preprocessed_data\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import cv2\n",
    "import os\n",
    "from glob import glob\n",
    "import re\n",
    "from mmsdk import mmdatasdk\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Paths\n",
    "mosi_path = \"/Users/camillebizeul/Downloads/Projet_Multi-20251201/MOSI\"\n",
    "output_path = \"/Users/camillebizeul/Downloads/Projet_Multi-20251201/preprocessed_data\"\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "print(f\"MOSI path: {mosi_path}\")\n",
    "print(f\"Output path: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afafa82b",
   "metadata": {},
   "source": [
    "## 1. Load MOSI Labels from SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4528b49a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading MOSI labels from SDK...\n",
      "\u001b[94m\u001b[1m[2025-12-02 10:09:52.041] | Status  | \u001b[0mDownloading from http://immortal.multicomp.cs.cmu.edu/CMU-MOSI/labels/CMU_MOSI_Opinion_Labels.csd to /Users/camillebizeul/Downloads/Projet_Multi-20251201/MOSI/SDK_data/CMU_MOSI_Opinion_Labels.csd...\n",
      "\u001b[94m\u001b[1m[2025-12-02 10:09:52.041] | Status  | \u001b[0mDownloading from http://immortal.multicomp.cs.cmu.edu/CMU-MOSI/labels/CMU_MOSI_Opinion_Labels.csd to /Users/camillebizeul/Downloads/Projet_Multi-20251201/MOSI/SDK_data/CMU_MOSI_Opinion_Labels.csd...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m\u001b[1m[2025-12-02 10:09:54.066] | Success | \u001b[0mDownload complete!\n",
      "\u001b[92m\u001b[1m[2025-12-02 10:09:54.070] | Success | \u001b[0mComputational sequence read from file /Users/camillebizeul/Downloads/Projet_Multi-20251201/MOSI/SDK_data/CMU_MOSI_Opinion_Labels.csd ...\n",
      "\u001b[94m\u001b[1m[2025-12-02 10:09:54.076] | Status  | \u001b[0mChecking the integrity of the <Opinion Segment Labels> computational sequence ...\n",
      "\u001b[94m\u001b[1m[2025-12-02 10:09:54.076] | Status  | \u001b[0mChecking the format of the data in <Opinion Segment Labels> computational sequence ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m\u001b[1m[2025-12-02 10:09:54.099] | Success | \u001b[0m<Opinion Segment Labels> computational sequence data in correct format.\n",
      "\u001b[94m\u001b[1m[2025-12-02 10:09:54.099] | Status  | \u001b[0mChecking the format of the metadata in <Opinion Segment Labels> computational sequence ...\n",
      "\u001b[93m\u001b[1m[2025-12-02 10:09:54.099] | Warning | \u001b[0m<Opinion Segment Labels> computational sequence does not have all the required metadata ... continuing \n",
      "\u001b[92m\u001b[1m[2025-12-02 10:09:54.099] | Success | \u001b[0mDataset initialized successfully ... \n",
      "✓ Loaded labels for 93 videos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# Load MOSI labels using SDK\n",
    "print(\"Loading MOSI labels from SDK...\")\n",
    "dataset = mmdatasdk.mmdataset(mmdatasdk.cmu_mosi.labels, os.path.join(mosi_path, 'SDK_data'))\n",
    "opinion_labels_sdk = dataset.computational_sequences['Opinion Segment Labels']\n",
    "\n",
    "print(f\"✓ Loaded labels for {len(opinion_labels_sdk.data)} videos\")\n",
    "\n",
    "def get_sentiment_label(video_id, segment_id):\n",
    "    \"\"\"Retrieve sentiment label from SDK.\"\"\"\n",
    "    try:\n",
    "        key = video_id\n",
    "        if key in opinion_labels_sdk.data:\n",
    "            intervals = opinion_labels_sdk.data[key]['intervals']\n",
    "            features = opinion_labels_sdk.data[key]['features']\n",
    "            segment_idx = int(segment_id) - 1\n",
    "            if 0 <= segment_idx < len(features):\n",
    "                sentiment_score = features[segment_idx][0]\n",
    "                return float(sentiment_score)\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb24ac6c",
   "metadata": {},
   "source": [
    "## 2. Initialize Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "27cec7cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT model...\n",
      "✓ BERT loaded\n",
      "✓ TextEncoder initialized (matches training architecture)\n",
      "✓ Image encoder initialized\n",
      "✓ BERT loaded\n",
      "✓ TextEncoder initialized (matches training architecture)\n",
      "✓ Image encoder initialized\n"
     ]
    }
   ],
   "source": [
    "# Initialize BERT\n",
    "print(\"Loading BERT model...\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "bert = BertModel.from_pretrained(\"bert-base-uncased\").to(device)\n",
    "bert.eval()\n",
    "print(\"✓ BERT loaded\")\n",
    "\n",
    "# TextEncoder class (matches training notebook architecture)\n",
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, bert_dim=768, num_output_tokens=8, nhead=8):\n",
    "        super().__init__()\n",
    "        self.bert_dim = bert_dim\n",
    "        self.T = num_output_tokens\n",
    "        \n",
    "        # Token spécial E_m (learnable)\n",
    "        self.special_token = nn.Parameter(torch.randn(1, 1, bert_dim))\n",
    "        \n",
    "        # TransformerEncoderLayer\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=bert_dim,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=2048,\n",
    "            dropout=0.1,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=1)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Obtenir la séquence brute de BERT\n",
    "        with torch.no_grad():\n",
    "            bert_output = bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            bert_sequence = bert_output.last_hidden_state  # [batch_size, seq_len, 768]\n",
    "        \n",
    "        batch_size = bert_sequence.size(0)\n",
    "        seq_len = bert_sequence.size(1)\n",
    "        \n",
    "        # Ajouter le token spécial E_m en tête\n",
    "        special_token_expanded = self.special_token.expand(batch_size, -1, -1)\n",
    "        sequence_with_token = torch.cat([special_token_expanded, bert_sequence], dim=1)\n",
    "        \n",
    "        # Appliquer le TransformerEncoderLayer\n",
    "        enhanced_sequence = self.transformer_encoder(sequence_with_token)\n",
    "        \n",
    "        # Ne conserver que les T=8 premiers tokens (ou padding si moins)\n",
    "        if enhanced_sequence.size(1) >= self.T:\n",
    "            output_sequence = enhanced_sequence[:, :self.T, :]  # [batch_size, 8, 768]\n",
    "        else:\n",
    "            # Si moins de T tokens, faire du padding\n",
    "            padding_size = self.T - enhanced_sequence.size(1)\n",
    "            padding = torch.zeros(batch_size, padding_size, self.bert_dim, device=enhanced_sequence.device)\n",
    "            output_sequence = torch.cat([enhanced_sequence, padding], dim=1)\n",
    "        \n",
    "        return output_sequence\n",
    "\n",
    "# Initialize TextEncoder\n",
    "text_encoder = TextEncoder().to(device)\n",
    "text_encoder.eval()\n",
    "print(\"✓ TextEncoder initialized (matches training architecture)\")\n",
    "\n",
    "# Simple Image Encoder\n",
    "class ImgEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(3, 8, 3, 1, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d(1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x / 255.\n",
    "        x = torch.tensor(x).permute(2, 0, 1).unsqueeze(0).float().to(device)\n",
    "        return self.model(x).flatten()\n",
    "\n",
    "img_encoder = ImgEncoder().to(device)\n",
    "img_encoder.eval()\n",
    "print(\"✓ Image encoder initialized\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a73d426",
   "metadata": {},
   "source": [
    "## 3. Feature Extraction Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d84d2782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Feature extraction functions defined (using TextEncoder for all text)\n"
     ]
    }
   ],
   "source": [
    "sr = 16000\n",
    "\n",
    "def extract_audio_features(audio, sr=16000):\n",
    "    \"\"\"Extract emotional audio features.\"\"\"\n",
    "    pitches, magnitudes = librosa.piptrack(y=audio, sr=sr)\n",
    "    pitch = np.mean([pitches[magnitudes[:, t].argmax(), t] \n",
    "                     for t in range(pitches.shape[1]) \n",
    "                     if magnitudes[:, t].max() > 0] or [0])\n",
    "    \n",
    "    rms = librosa.feature.rms(y=audio)[0]\n",
    "    loudness = np.mean(rms)\n",
    "    \n",
    "    pitch_values = [pitches[magnitudes[:, t].argmax(), t] \n",
    "                    for t in range(pitches.shape[1]) \n",
    "                    if magnitudes[:, t].max() > 0]\n",
    "    jitter = np.std(pitch_values) if len(pitch_values) > 1 else 0\n",
    "    shimmer = np.std(rms)\n",
    "    \n",
    "    return {\n",
    "        'pitch': float(pitch),\n",
    "        'loudness': float(loudness),\n",
    "        'jitter': float(jitter),\n",
    "        'shimmer': float(shimmer)\n",
    "    }\n",
    "\n",
    "def audio_description(pitch, loudness, jitter, shimmer):\n",
    "    \"\"\"Convert audio features to emotional description.\"\"\"\n",
    "    pitch_level = \"high\" if pitch > 200 else \"low\" if pitch > 100 else \"very low\"\n",
    "    loudness_level = \"loud\" if loudness > 0.1 else \"moderate\" if loudness > 0.05 else \"quiet\"\n",
    "    jitter_level = \"high variation\" if jitter > 20 else \"stable\"\n",
    "    shimmer_level = \"variable amplitude\" if shimmer > 0.02 else \"steady amplitude\"\n",
    "    \n",
    "    return (\n",
    "        f\"The speaker used {pitch_level} pitch with {loudness_level} volume. \"\n",
    "        f\"The voice shows {jitter_level} in pitch and {shimmer_level}.\"\n",
    "    )\n",
    "\n",
    "AU_DESCRIPTIONS = {\n",
    "    'AU1': 'raised inner brow', 'AU2': 'raised outer brow', 'AU4': 'lowered brow',\n",
    "    'AU5': 'upper lid raise', 'AU6': 'cheek raise', 'AU7': 'lid tightener',\n",
    "    'AU9': 'nose wrinkle', 'AU10': 'upper lip raise', 'AU12': 'lip corner pull',\n",
    "    'AU15': 'lip corner depress', 'AU17': 'chin raise', 'AU20': 'lip stretch',\n",
    "    'AU23': 'lip tightener', 'AU25': 'lips part', 'AU26': 'jaw drop', 'AU45': 'blink'\n",
    "}\n",
    "\n",
    "def visual_description(aus):\n",
    "    \"\"\"Convert Action Units to emotional description.\"\"\"\n",
    "    if isinstance(aus, dict):\n",
    "        active_aus = [au for au, active in aus.items() if active]\n",
    "    else:\n",
    "        active_aus = aus\n",
    "    \n",
    "    if not active_aus:\n",
    "        return \"The person shows a neutral expression with no significant facial movement.\"\n",
    "    \n",
    "    descriptions = [AU_DESCRIPTIONS.get(au, au) for au in active_aus]\n",
    "    \n",
    "    if len(descriptions) == 1:\n",
    "        return f\"The person shows signs of: {descriptions[0]}.\"\n",
    "    elif len(descriptions) == 2:\n",
    "        return f\"The person shows signs of: {descriptions[0]} and {descriptions[1]}.\"\n",
    "    else:\n",
    "        desc_list = \", \".join(descriptions[:-1]) + f\", and {descriptions[-1]}\"\n",
    "        return f\"The person shows signs of: {desc_list}.\"\n",
    "\n",
    "def encode_text_with_text_encoder(text):\n",
    "    \"\"\"\n",
    "    Encode text using TextEncoder (matches training pipeline).\n",
    "    Returns mean-pooled embedding from the enhanced sequence.\n",
    "    \"\"\"\n",
    "    tokens = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=128).to(device)\n",
    "    with torch.no_grad():\n",
    "        # Use TextEncoder to get enhanced sequence [1, 8, 768]\n",
    "        sequence = text_encoder(tokens['input_ids'], tokens['attention_mask'])\n",
    "        # Mean pool the sequence to get single embedding [1, 768]\n",
    "        emb = sequence.mean(dim=1)\n",
    "    return emb.cpu().numpy().flatten()\n",
    "\n",
    "def encode_audio_mfcc(audio):\n",
    "    \"\"\"Extract MFCC features from audio.\"\"\"\n",
    "    mfcc = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=20)\n",
    "    return mfcc.mean(axis=1)\n",
    "\n",
    "print(\"✓ Feature extraction functions defined (using TextEncoder for all text)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c56b2b",
   "metadata": {},
   "source": [
    "## 4. Data Loading & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "65baf833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Data preprocessing function defined (uses TextEncoder for all text embeddings)\n"
     ]
    }
   ],
   "source": [
    "def parse_transcript_file(filepath):\n",
    "    \"\"\"Parse transcript file.\"\"\"\n",
    "    segments = []\n",
    "    with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if '_DELIM_' in line:\n",
    "                parts = line.split('_DELIM_', 1)\n",
    "                if len(parts) == 2:\n",
    "                    seg_id = parts[0].strip()\n",
    "                    text = parts[1].strip()\n",
    "                    segments.append({'id': seg_id, 'text': text})\n",
    "    return segments\n",
    "\n",
    "def extract_video_id(filename):\n",
    "    \"\"\"Extract video ID from filename.\"\"\"\n",
    "    base = os.path.splitext(os.path.basename(filename))[0]\n",
    "    parts = base.rsplit('_', 1)\n",
    "    return parts[0] if len(parts) > 1 else base\n",
    "\n",
    "def preprocess_mosi_dataset(max_samples=500):\n",
    "    \"\"\"Preprocess MOSI dataset and extract all features using TextEncoder.\"\"\"\n",
    "    \n",
    "    audio_dir = os.path.join(mosi_path, \"Audio\", \"WAV_16000\", \"Segmented\")\n",
    "    video_dir = os.path.join(mosi_path, \"Video\", \"Segmented\")\n",
    "    transcript_dir = os.path.join(mosi_path, \"Transcript\", \"Segmented\")\n",
    "    \n",
    "    audio_files = sorted(glob(os.path.join(audio_dir, \"*.wav\")))\n",
    "    \n",
    "    data_records = []\n",
    "    \n",
    "    print(f\"\\nProcessing up to {max_samples} samples...\")\n",
    "    \n",
    "    for audio_file in tqdm(audio_files[:max_samples], desc=\"Processing samples\"):\n",
    "        try:\n",
    "            basename = os.path.basename(audio_file)\n",
    "            base_no_ext = os.path.splitext(basename)[0]\n",
    "            video_id = extract_video_id(audio_file)\n",
    "            \n",
    "            seg_match = re.search(r'_(\\d+)$', base_no_ext)\n",
    "            if not seg_match:\n",
    "                continue\n",
    "            seg_id = seg_match.group(1)\n",
    "            \n",
    "            # Get label\n",
    "            sentiment_label = get_sentiment_label(video_id, seg_id)\n",
    "            if sentiment_label is None:\n",
    "                continue\n",
    "            \n",
    "            # Load and process audio\n",
    "            audio, _ = librosa.load(audio_file, sr=sr)\n",
    "            audio_duration = len(audio) / sr\n",
    "            \n",
    "            # Extract audio features\n",
    "            audio_mfcc = encode_audio_mfcc(audio)\n",
    "            audio_emotional_features = extract_audio_features(audio, sr)\n",
    "            audio_desc = audio_description(\n",
    "                audio_emotional_features['pitch'],\n",
    "                audio_emotional_features['loudness'],\n",
    "                audio_emotional_features['jitter'],\n",
    "                audio_emotional_features['shimmer']\n",
    "            )\n",
    "            # Use TextEncoder for audio description\n",
    "            audio_desc_embedding = encode_text_with_text_encoder(audio_desc)\n",
    "            \n",
    "            # Load and process video\n",
    "            video_file = os.path.join(video_dir, f\"{video_id}_{seg_id}.mp4\")\n",
    "            if os.path.exists(video_file):\n",
    "                cap = cv2.VideoCapture(video_file)\n",
    "                frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "                if frame_count > 0:\n",
    "                    cap.set(cv2.CAP_PROP_POS_FRAMES, frame_count // 2)\n",
    "                    ret, frame = cap.read()\n",
    "                    if ret:\n",
    "                        image = cv2.resize(frame, (64, 64))\n",
    "                        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "                    else:\n",
    "                        image = np.zeros((64, 64, 3), dtype=np.uint8)\n",
    "                else:\n",
    "                    image = np.zeros((64, 64, 3), dtype=np.uint8)\n",
    "                cap.release()\n",
    "            else:\n",
    "                image = np.zeros((64, 64, 3), dtype=np.uint8)\n",
    "            \n",
    "            # Extract visual features\n",
    "            visual_cnn = img_encoder(image).detach().cpu().numpy()\n",
    "            \n",
    "            # Simulate AUs based on image intensity\n",
    "            mean_intensity = np.mean(image)\n",
    "            if mean_intensity > 170:\n",
    "                aus = ['AU6', 'AU12']\n",
    "            elif mean_intensity > 85:\n",
    "                aus = ['AU1', 'AU2']\n",
    "            else:\n",
    "                aus = ['AU4', 'AU15']\n",
    "            \n",
    "            visual_desc = visual_description(aus)\n",
    "            # Use TextEncoder for visual description\n",
    "            visual_desc_embedding = encode_text_with_text_encoder(visual_desc)\n",
    "            \n",
    "            # Load transcript\n",
    "            transcript_file = os.path.join(transcript_dir, f\"{video_id}.annotprocessed\")\n",
    "            text = \"\"\n",
    "            if os.path.exists(transcript_file):\n",
    "                segments = parse_transcript_file(transcript_file)\n",
    "                for seg in segments:\n",
    "                    if seg['id'] == seg_id:\n",
    "                        text = seg['text']\n",
    "                        break\n",
    "            if not text:\n",
    "                text = f\"Segment {seg_id}\"\n",
    "            \n",
    "            # Encode text using TextEncoder\n",
    "            text_embedding = encode_text_with_text_encoder(text)\n",
    "            \n",
    "            # Create record\n",
    "            record = {\n",
    "                'video_id': video_id,\n",
    "                'segment_id': seg_id,\n",
    "                'text': text,\n",
    "                'text_embedding': text_embedding,\n",
    "                'audio_mfcc': audio_mfcc,\n",
    "                'audio_pitch': audio_emotional_features['pitch'],\n",
    "                'audio_loudness': audio_emotional_features['loudness'],\n",
    "                'audio_jitter': audio_emotional_features['jitter'],\n",
    "                'audio_shimmer': audio_emotional_features['shimmer'],\n",
    "                'audio_description': audio_desc,\n",
    "                'audio_desc_embedding': audio_desc_embedding,\n",
    "                'visual_cnn': visual_cnn,\n",
    "                'visual_description': visual_desc,\n",
    "                'visual_desc_embedding': visual_desc_embedding,\n",
    "                'audio_duration': audio_duration,\n",
    "                'sentiment_label': sentiment_label\n",
    "            }\n",
    "            \n",
    "            data_records.append(record)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\nError processing {basename}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return data_records\n",
    "\n",
    "print(\"✓ Data preprocessing function defined (uses TextEncoder for all text embeddings)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065d40c7",
   "metadata": {},
   "source": [
    "## 5. Process Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3ce626a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "STARTING DATA PREPROCESSING\n",
      "======================================================================\n",
      "\n",
      "Processing up to 500 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing samples: 100%|██████████| 500/500 [00:49<00:00, 10.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Processed 500 samples successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Process the dataset (adjust max_samples as needed)\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STARTING DATA PREPROCESSING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "data_records = preprocess_mosi_dataset(max_samples=500)\n",
    "\n",
    "print(f\"\\n✓ Processed {len(data_records)} samples successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6fd3a4",
   "metadata": {},
   "source": [
    "## 6. Create DataFrame & Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0042e8d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "DATASET STATISTICS\n",
      "======================================================================\n",
      "Total samples: 500\n",
      "\n",
      "Columns: ['video_id', 'segment_id', 'text', 'text_embedding', 'audio_mfcc', 'audio_pitch', 'audio_loudness', 'audio_jitter', 'audio_shimmer', 'audio_description', 'audio_desc_embedding', 'visual_cnn', 'visual_description', 'visual_desc_embedding', 'audio_duration', 'sentiment_label']\n",
      "\n",
      "Sentiment label statistics:\n",
      "  Min: -2.800\n",
      "  Max: 3.000\n",
      "  Mean: 0.417\n",
      "  Std: 1.451\n",
      "\n",
      "Label distribution:\n",
      "  Positive (>0): 280\n",
      "  Negative (<0): 197\n",
      "  Neutral (=0): 23\n",
      "\n",
      "======================================================================\n",
      "SAMPLE DATA (first 3 rows)\n",
      "======================================================================\n",
      "\n",
      "Sample 1:\n",
      "  Video ID: 03bSnISJMiM\n",
      "  Segment ID: 1\n",
      "  Text: ANYHOW IT WAS REALLY GOOD...\n",
      "  Audio Description: The speaker used high pitch with quiet volume. The voice shows high variation in pitch and steady amplitude.\n",
      "  Visual Description: The person shows signs of: lowered brow and lip corner depress.\n",
      "  Sentiment Label: +2.400\n",
      "  Text embedding shape: (768,)\n",
      "  Audio MFCC shape: (20,)\n",
      "  Visual CNN shape: (8,)\n",
      "\n",
      "Sample 2:\n",
      "  Video ID: 03bSnISJMiM\n",
      "  Segment ID: 10\n",
      "  Text: THERE IS SAD PART...\n",
      "  Audio Description: The speaker used high pitch with quiet volume. The voice shows high variation in pitch and steady amplitude.\n",
      "  Visual Description: The person shows signs of: lowered brow and lip corner depress.\n",
      "  Sentiment Label: -1.200\n",
      "  Text embedding shape: (768,)\n",
      "  Audio MFCC shape: (20,)\n",
      "  Visual CNN shape: (8,)\n",
      "\n",
      "Sample 3:\n",
      "  Video ID: 03bSnISJMiM\n",
      "  Segment ID: 11\n",
      "  Text: A LOT OF SAD PARTS...\n",
      "  Audio Description: The speaker used high pitch with quiet volume. The voice shows high variation in pitch and steady amplitude.\n",
      "  Visual Description: The person shows signs of: lowered brow and lip corner depress.\n",
      "  Sentiment Label: -0.500\n",
      "  Text embedding shape: (768,)\n",
      "  Audio MFCC shape: (20,)\n",
      "  Visual CNN shape: (8,)\n"
     ]
    }
   ],
   "source": [
    "# Create DataFrame\n",
    "df = pd.DataFrame(data_records)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DATASET STATISTICS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Total samples: {len(df)}\")\n",
    "print(f\"\\nColumns: {list(df.columns)}\")\n",
    "print(f\"\\nSentiment label statistics:\")\n",
    "print(f\"  Min: {df['sentiment_label'].min():.3f}\")\n",
    "print(f\"  Max: {df['sentiment_label'].max():.3f}\")\n",
    "print(f\"  Mean: {df['sentiment_label'].mean():.3f}\")\n",
    "print(f\"  Std: {df['sentiment_label'].std():.3f}\")\n",
    "print(f\"\\nLabel distribution:\")\n",
    "print(f\"  Positive (>0): {(df['sentiment_label'] > 0).sum()}\")\n",
    "print(f\"  Negative (<0): {(df['sentiment_label'] < 0).sum()}\")\n",
    "print(f\"  Neutral (=0): {(df['sentiment_label'] == 0).sum()}\")\n",
    "\n",
    "# Display sample\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SAMPLE DATA (first 3 rows)\")\n",
    "print(\"=\"*70)\n",
    "for idx in range(min(3, len(df))):\n",
    "    print(f\"\\nSample {idx+1}:\")\n",
    "    print(f\"  Video ID: {df.iloc[idx]['video_id']}\")\n",
    "    print(f\"  Segment ID: {df.iloc[idx]['segment_id']}\")\n",
    "    print(f\"  Text: {df.iloc[idx]['text'][:80]}...\")\n",
    "    print(f\"  Audio Description: {df.iloc[idx]['audio_description']}\")\n",
    "    print(f\"  Visual Description: {df.iloc[idx]['visual_description']}\")\n",
    "    print(f\"  Sentiment Label: {df.iloc[idx]['sentiment_label']:+.3f}\")\n",
    "    print(f\"  Text embedding shape: {df.iloc[idx]['text_embedding'].shape}\")\n",
    "    print(f\"  Audio MFCC shape: {df.iloc[idx]['audio_mfcc'].shape}\")\n",
    "    print(f\"  Visual CNN shape: {df.iloc[idx]['visual_cnn'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "34da80d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "SAVING PREPROCESSED DATA\n",
      "======================================================================\n",
      "✓ Saved pickle file: /Users/camillebizeul/Downloads/Projet_Multi-20251201/preprocessed_data/mosi_preprocessed.pkl\n",
      "✓ Saved metadata CSV: /Users/camillebizeul/Downloads/Projet_Multi-20251201/preprocessed_data/mosi_metadata.csv\n",
      "✓ Saved dataset info: /Users/camillebizeul/Downloads/Projet_Multi-20251201/preprocessed_data/dataset_info.pkl\n",
      "\n",
      "======================================================================\n",
      "PREPROCESSING COMPLETE!\n",
      "======================================================================\n",
      "\n",
      "Files saved in: /Users/camillebizeul/Downloads/Projet_Multi-20251201/preprocessed_data\n",
      "  - mosi_preprocessed.pkl (full dataset with embeddings)\n",
      "  - mosi_metadata.csv (metadata without embeddings)\n",
      "  - dataset_info.pkl (feature dimensions and info)\n"
     ]
    }
   ],
   "source": [
    "# Save DataFrame\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SAVING PREPROCESSED DATA\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Save as pickle (preserves numpy arrays)\n",
    "pickle_path = os.path.join(output_path, 'mosi_preprocessed.pkl')\n",
    "with open(pickle_path, 'wb') as f:\n",
    "    pickle.dump(df, f)\n",
    "print(f\"✓ Saved pickle file: {pickle_path}\")\n",
    "\n",
    "# Save metadata as CSV (without embeddings)\n",
    "metadata_df = df[['video_id', 'segment_id', 'text', 'audio_description', 'visual_description', \n",
    "                  'audio_pitch', 'audio_loudness', 'audio_jitter', 'audio_shimmer',\n",
    "                  'audio_duration', 'sentiment_label']].copy()\n",
    "csv_path = os.path.join(output_path, 'mosi_metadata.csv')\n",
    "metadata_df.to_csv(csv_path, index=False)\n",
    "print(f\"✓ Saved metadata CSV: {csv_path}\")\n",
    "\n",
    "# Save feature dimensions info\n",
    "info = {\n",
    "    'num_samples': len(df),\n",
    "    'text_embedding_dim': df.iloc[0]['text_embedding'].shape[0],\n",
    "    'audio_mfcc_dim': df.iloc[0]['audio_mfcc'].shape[0],\n",
    "    'visual_cnn_dim': df.iloc[0]['visual_cnn'].shape[0],\n",
    "    'audio_desc_embedding_dim': df.iloc[0]['audio_desc_embedding'].shape[0],\n",
    "    'visual_desc_embedding_dim': df.iloc[0]['visual_desc_embedding'].shape[0],\n",
    "    'columns': list(df.columns)\n",
    "}\n",
    "\n",
    "info_path = os.path.join(output_path, 'dataset_info.pkl')\n",
    "with open(info_path, 'wb') as f:\n",
    "    pickle.dump(info, f)\n",
    "print(f\"✓ Saved dataset info: {info_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PREPROCESSING COMPLETE!\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nFiles saved in: {output_path}\")\n",
    "print(f\"  - mosi_preprocessed.pkl (full dataset with embeddings)\")\n",
    "print(f\"  - mosi_metadata.csv (metadata without embeddings)\")\n",
    "print(f\"  - dataset_info.pkl (feature dimensions and info)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "34d76a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraction et génération des features AED/VED pour chaque échantillon\n",
    "# Fonctionne même si 'audio'/'aus' ne sont pas présents dans df\n",
    "preprocessed_audio_desc = []\n",
    "preprocessed_visual_desc = []\n",
    "\n",
    "has_audio = 'audio' in df.columns\n",
    "has_aus = 'aus' in df.columns\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    # Audio description\n",
    "    if has_audio:\n",
    "        feats = extract_audio_features(row['audio'], sr=sr)  # retourne un dict\n",
    "        aed = audio_description(feats['pitch'], feats['loudness'], feats['jitter'], feats['shimmer'])\n",
    "    elif all(c in df.columns for c in ['audio_pitch','audio_loudness','audio_jitter','audio_shimmer']):\n",
    "        aed = audio_description(row['audio_pitch'], row['audio_loudness'], row['audio_jitter'], row['audio_shimmer'])\n",
    "    else:\n",
    "        aed = row['audio_description'] if 'audio_description' in df.columns else \"\"\n",
    "    preprocessed_audio_desc.append(aed)\n",
    "\n",
    "    # Visual description\n",
    "    if has_aus:\n",
    "        ved = visual_description(row['aus'])\n",
    "    else:\n",
    "        ved = row['visual_description'] if 'visual_description' in df.columns else \"\"\n",
    "    preprocessed_visual_desc.append(ved)\n",
    "\n",
    "# Ajout des colonnes harmonisées\n",
    "df['audio_desc'] = preprocessed_audio_desc\n",
    "df['visual_desc'] = preprocessed_visual_desc\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projet-multi-20251201",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
